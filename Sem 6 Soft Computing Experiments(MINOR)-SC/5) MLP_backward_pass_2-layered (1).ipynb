{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520c74f4-7770-4a04-a2ae-81449b539179",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1744870495967,
     "user": {
      "displayName": "Vaibhav Chavan",
      "userId": "10537540842134536366"
     },
     "user_tz": -330
    },
    "id": "520c74f4-7770-4a04-a2ae-81449b539179",
    "outputId": "d12bd4d8-dfd1-4055-9bad-156836cef72f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH- 1 ================================================================================\n",
      "\n",
      "ITER-1 --------------------------------------------------------------------------------\n",
      "Bias added to A_0\n",
      "Bias added to A_1\n",
      "A_0 (input):\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Z_1 (hidden pre-activation):\n",
      "[[ 1.]\n",
      " [-2.]]\n",
      "A_1 (hidden post-activation):\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [-2.]]\n",
      "Z_2 (output pre-activation):\n",
      "[[3.]]\n",
      "A_2 (output post-activation):\n",
      "[[3.]]\n",
      "Bias column removed from W_1 for backpropagation.\n",
      "[[ 1. -1.]]\n",
      "dW_1:\n",
      "[[ 3.  3. -6.]]\n",
      "dW_0:\n",
      "[[ 3.  3.  0.]\n",
      " [-3. -3.  0.]]\n",
      "Updated W_1:\n",
      "[[-3. -2.  5.]]\n",
      "Updated W_0:\n",
      "[[-2. -3.  1.]\n",
      " [ 2.  2.  1.]]\n",
      "\n",
      "ITER-2 --------------------------------------------------------------------------------\n",
      "Bias added to A_0\n",
      "Bias added to A_1\n",
      "A_0 (input):\n",
      "[[ 1.]\n",
      " [-1.]\n",
      " [-1.]]\n",
      "Z_1 (hidden pre-activation):\n",
      "[[ 0.]\n",
      " [-1.]]\n",
      "A_1 (hidden post-activation):\n",
      "[[ 1.]\n",
      " [ 0.]\n",
      " [-1.]]\n",
      "Z_2 (output pre-activation):\n",
      "[[-8.]]\n",
      "A_2 (output post-activation):\n",
      "[[0.]]\n",
      "Bias column removed from W_1 for backpropagation.\n",
      "[[-2.  5.]]\n",
      "dW_1:\n",
      "[[0. 0. 0.]]\n",
      "dW_0:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Updated W_1:\n",
      "[[-3. -2.  5.]]\n",
      "Updated W_0:\n",
      "[[-2. -3.  1.]\n",
      " [ 2.  2.  1.]]\n",
      "\n",
      "ITER-3 --------------------------------------------------------------------------------\n",
      "Bias added to A_0\n",
      "Bias added to A_1\n",
      "A_0 (input):\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Z_1 (hidden pre-activation):\n",
      "[[-4.]\n",
      " [ 5.]]\n",
      "A_1 (hidden post-activation):\n",
      "[[ 1.]\n",
      " [-4.]\n",
      " [ 5.]]\n",
      "Z_2 (output pre-activation):\n",
      "[[30.]]\n",
      "A_2 (output post-activation):\n",
      "[[30.]]\n",
      "Bias column removed from W_1 for backpropagation.\n",
      "[[-2.  5.]]\n",
      "dW_1:\n",
      "[[  29. -116.  145.]]\n",
      "dW_0:\n",
      "[[-58. -58. -58.]\n",
      " [145. 145. 145.]]\n",
      "Updated W_1:\n",
      "[[ -32.  114. -140.]]\n",
      "Updated W_0:\n",
      "[[  56.   55.   59.]\n",
      " [-143. -143. -144.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Initialize weights\n",
    "W_0 = np.array([[1, 0, 1],\n",
    "                [-1, -1, 1]], dtype=float)  # Hidden layer weights\n",
    "W_1 = np.array([[0, 1, -1]], dtype=float)   # Output layer weights\n",
    "\n",
    "# Input and target\n",
    "X = np.array([[1, -1, 1],\n",
    "              [0, -1, 1]], dtype=float)     # Shape: (2, 2)\n",
    "t = np.array([[0, 0, 1]], dtype=float)   # Shape: (1, 2)\n",
    "\n",
    "# Hyperparameters\n",
    "f_1 = \"Lin\"\n",
    "f_2 = \"ReLU\"\n",
    "lr = 1\n",
    "MAX_EPOCHS = 1\n",
    "\n",
    "# Activation Functions\n",
    "def USigmoid(x, direction='F'):\n",
    "    fx = 1 / (1 + np.exp(-x))\n",
    "    return fx if direction == 'F' else fx * (1 - fx)\n",
    "\n",
    "def BSigmoid(x, direction='F'):\n",
    "    fx = (1 - np.exp(-x)) / (1 + np.exp(-x))\n",
    "    return fx if direction == 'F' else 0.5 * (1 - fx ** 2)\n",
    "\n",
    "def TanH(x, direction='F'):\n",
    "    fx = np.tanh(x)\n",
    "    return fx if direction == 'F' else 1 - fx ** 2\n",
    "\n",
    "def ReLU(x, direction='F'):\n",
    "    return np.maximum(0, x) if direction == 'F' else (x > 0).astype(float)\n",
    "\n",
    "def Lin(x, direction='F'):\n",
    "    return x if direction == 'F' else np.ones_like(x)\n",
    "\n",
    "# Wrapper for activation\n",
    "def activation(Z, fcn=\"Lin\", direction='F'):\n",
    "    return np.array([globals()[fcn](z, direction) for z in Z]).reshape(-1, 1)\n",
    "\n",
    "# A_0 = np.vstack((np.ones((1, 1)), x.reshape(-1, 1)))\n",
    "\n",
    "# Training loop\n",
    "for ep in range(MAX_EPOCHS):\n",
    "    print('\\nEPOCH-', ep + 1, '=' * 80)\n",
    "    for itr, (x, y) in enumerate(zip(X.T, t.T)):\n",
    "        print(f'\\nITER-{itr + 1} ' + '-' * 80)\n",
    "\n",
    "        # Forward Pass: Input -> Hidden\n",
    "        A_0 = x.reshape(-1, 1)\n",
    "\n",
    "        # Conditionally add bias to A_0\n",
    "        if W_0.shape[1] == A_0.shape[0] + 1:\n",
    "            A_0 = np.vstack((np.ones((1, 1)), A_0))  # Add bias at the top\n",
    "            print('Bias added to A_0')\n",
    "\n",
    "        Z_1 = W_0 @ A_0\n",
    "        A_1 = activation(Z_1, f_1)\n",
    "\n",
    "        # Forward Pass: Hidden -> Output\n",
    "        # Conditionally add bias to A_1\n",
    "        if W_1.shape[1] == A_1.shape[0] + 1:\n",
    "            A_1 = np.vstack((np.ones((1, 1)), A_1))  # Add bias at the top\n",
    "            print('Bias added to A_1')\n",
    "\n",
    "        Z_2 = W_1 @ A_1\n",
    "        A_2 = activation(Z_2, f_2)\n",
    "\n",
    "        print(f'A_0 (input):\\n{A_0}')\n",
    "        print(f'Z_1 (hidden pre-activation):\\n{Z_1}')\n",
    "        print(f'A_1 (hidden post-activation):\\n{A_1}')\n",
    "        print(f'Z_2 (output pre-activation):\\n{Z_2}')\n",
    "        print(f'A_2 (output post-activation):\\n{A_2}')\n",
    "\n",
    "       # Backward Pass: Output -> Hidden\n",
    "        delta_2 = (A_2 - y.reshape(-1, 1)) * activation(Z_2, f_2, 'B')\n",
    "        dW_1 = delta_2 @ A_1.T\n",
    "\n",
    "        # Remove bias from W_1 if necessary\n",
    "        if W_1.shape[1] == Z_1.shape[0] + 1:\n",
    "            W_1_no_bias = W_1[:, 1:]\n",
    "            print(\"Bias column removed from W_1 for backpropagation.\")\n",
    "        else:\n",
    "            W_1_no_bias = W_1\n",
    "\n",
    "        print(W_1_no_bias)\n",
    "\n",
    "        # Backward Pass: Hidden -> Input\n",
    "        delta_1 = (W_1_no_bias.T @ delta_2) * activation(Z_1, f_1, 'B')\n",
    "        dW_0 = delta_1 @ A_0.T\n",
    "\n",
    "        # Update weights\n",
    "        W_1 -= lr * dW_1\n",
    "        W_0 -= lr * dW_0\n",
    "\n",
    "        print(f'dW_1:\\n{dW_1}')\n",
    "        print(f'dW_0:\\n{dW_0}')\n",
    "        print(f'Updated W_1:\\n{W_1}')\n",
    "        print(f'Updated W_0:\\n{W_0}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
